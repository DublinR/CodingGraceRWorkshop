%======================================================================================================== %
\section*{7. Polynomial Regression}


The last method we'll use to fit these data is polynomial regression, where the model takes on the form y = b0 + b1x + b2x2 + b3x3 + ... , etc. We're going to fit a second order polynomial like this: 
\begin{framed}
\begin{verbatim}
> polyfit2 <- lm(vel~conc+I(conc^2), data=df)
\end{verbatim}
\end{framed}
We're using the same function lm() as linear regression, but we're adding multiple instances of the explanatory variable to generate our polynomial formula. Note that we need to use the identity function, I(), because the caret (^) has special meaning in a formula. Another way to build this formula is to create a matrix where each column contains the explanatory variable raised to a power. Use the function cbind() to bind columns together to form a matrix. This is what that would look like: 
> polyfit2a <- lm(vel~cbind(conc, conc^2), data=df)
Run either of these commands and view the summary or just take a look at the coefficients: 
\begin{framed}
\begin{verbatim}
> coef2 <- coef(polyfit2)
> coef2
(Intercept)        conc   I(conc^2) 
   1.288439   25.652243  -56.500264 
   \end{verbatim}
\end{framed}
Now we want to draw this line on our graph. We could add it to the plot with the other two lines, but let's create a new graph and label the x and y axes: 
> plot(df$conc, df$vel, xlim=c(0,0.4), ylim=c(0,5), 
+ xlab="Substrate Concentration", ylab="Reaction Rate")
There's something new with this line. You can enter the entire command on a single line if you want, but if you hit Enter before the command is complete, you get the "+" prompt on the second line where you finish the command. NOTE: the "+" on the second line IS NOT part of the command, it is the prompt to continue. So if you enter this all together on a single line, DO NOT include the "+". 
There are a couple ways to generate the y-values for the line. Perhaps the most straightforward is the following: 
> y3 <- coef2[1] + coef2[2]*x + coef2[3]*x^2
We just plug in the coefficients and the appropriate x-values and we're done. There's another way that doesn't involve so much typing, especially when dealing with higher order polynomials. It involves matrix multiplication so hopefully you remember something about linear algebra. We're going to create a matrix of x-values and then multiply that by our coefficient vector. 
\begin{framed}
\begin{verbatim}
> y3 <- cbind(1,x,x^2) %*% coef2
You saw the function cbind() just a second ago. (FYI: There is also a function called rbind() that binds vectors together as rows instead of columns.) The operator %*% is used for matrix multiplication. Add the line to the graph: 
> lines(x,y3)
And it should look like this: 

The last thing we're going to do is increase the polynomial order to the maximum. There are seven data points so the maximum order is six. (Why?) First we fit the new regression, then transform the coefficients, generate new y-values, and add the new line to our graph. 
> polyfit6 <- lm(vel~conc+I(conc^2)+I(conc^3)+I(conc^4)+I(conc^5)+I(conc^6),data=df)
> coef6 <- coef(polyfit6)
> y4 <- cbind(1,x,x^2,x^3,x^4,x^5,x^6) %*% coef6
> lines(x,y4,lty=2)
When you add the lines, you will see several warnings go by because some of the resulting y-values greatly exceed the range of the graph. The plot should now look like this: 

It's a good fit (the line goes through every point) but how useful is it for predicting new points? Take a look at the summaries from each fit. Ever seen an R2 of 1 before? 

