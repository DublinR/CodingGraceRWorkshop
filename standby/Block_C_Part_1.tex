



<p>
### {Correlation and Simple Regression Models}

<p>
#### {Correlation}

A correlation coefficient is a number between -1 and 1 which measures the degree to which two variables are linearly related. If there is perfect linear relationship with positive slope between the two variables, we have a correlation coefficient of 1; if there is positive correlation, whenever one variable has a high (low) value, so does the other.

If there is a perfect linear relationship with negative slope between the two variables, we have a correlation coefficient of -1; if there is negative correlation, whenever one variable has a high (low) value, the other has a low (high) value.
A correlation coefficient of 0 means that there is no linear relationship between the variables.

We can determine the Pearson Correlation coefficient in R using the texttt{cor()} command.
To get a more complete statistical analysis, with formal tests, we can use the command texttt{cor.test()}
The interpretation of the output from the cor.test()procedure is very similar to procedures we have already encountered. The null hypothesis is that the correlation coefficient is equal to zero. This is equivalent to saying that there is no linear relationship between variables.


<pre>
<code>
C=c(0,2,4,6,8,10,12) 
F=c(2.1,5.0,9.0,12.6,17.3,21.0,24.7)
cor.test(C,F)
</code>
</pre>
<p>
<code>

 Pearson's product-moment correlation

data:  C and F 
t = 47.1967, df = 5, p-value = 8.066e-08
alternative hypothesis: true correlation is not equal to 0 
95 percent confidence interval:
 0.9920730 0.9998421 
sample estimates:
      cor 
0.9988796 
</code>


<p>
#### {Spearman and Kendall Correlation}
Spearman and Kendall correlations are both textbf{emph{rank correlations}}. 
To implement Spearman and Kendall correlation, simply specify the type in the texttt{method=" "} argument.
<code>
> cor(G,D)
[1] 0.3167869
>
> cor(G,D,method="spearman")
[1] 0.1785714
>
> cor(G,D,method="kendall")
[1] 0.1428571
> 
</code>
The interpretation is very similar, but there are no confidence intervals for the estimates.

<p>
#### {Fitting a Regression Model}
A regression model is fitted using the texttt{lm()} command.

Consider the response variable $F$ and predictor variable $C$.
<pre>
<code>
C=c(0,2,4,6,8,10,12) 
F=c(2.1,5.0,9.0,12.6,17.3,21.0,24.7)
Fit1=lm(F~C)
</code>
</pre>
<p>


<p>
#### {Confidence and Prediction Intervals for Fitted Values} 

Recall that a fitted value $hat{Y}$ is a estimate for the response variable, as determined by a linear model. The difference between the observed value and the corresponding fitted value is known as the residual.

The textbf{emph{residual standard error}} is the conditional standard deviation of the dependent variable Y given a value of the independent variable X. The calculation of this standard error follows from the definition of the residuals.

The residual standard error is often called the root mean square error (RMSE), and is a measure of the differences between values predicted by a model or an estimator and the values actually observed from the thing being modelled or estimated.

Since the residual standard error is a good measure of accuracy, it is ideal if it is small.


#### {Prediction Intervals}
In contrast to a confidence interval, which is concerned with estimating a population parameter, a prediction interval is concerned with estimating an individual value and is therefore a type of probability interval. 

The complete standard error for a prediction interval is called the standard error of forecast, and it includes the uncertainty associated with the vertical “scatter” about the regression line plus the uncertainty associated with the position of the regression line value itself.










%----------------------------------------------------------------------------------------------------------------------------%
<p>
### {Working with Categorical Data}
<p>
#### {Chi-Square}

The table below shows the relationship between gender and party identification in a US state.


%	   & Democrat &	Independent & Republican & Total 
%Male   &	279	& 73  &	225 &	577 
%Female &	165	& 47  & 191 &	403 
%Total  &	444 & 120 &	416	&   980 

Test for association between gender and party affiliation at two appropriate levels
and comment on your results.

Set out the null hypothesis that there is no association between method of computation
and gender against the alternative, that there is. Be careful to get these the correct way
round!

H0: There is no association.
H1: There is an association.

Work out the expected values. For example, you should work out the expected value for
the number of males who use no aids from the following: (95/195) × 22 = 10.7.


<p>
### {Probability Distributions}
<p>
#### {Discrete Probability Distribution}


The two most accessible discrete distributions are the binomial and textbf{emph{Poisson}} distributions


