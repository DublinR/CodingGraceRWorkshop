\documentclass[pdf,default,slideColor,colorBG]{prosper}



% define a new font called goodfont
\def\goodfont{\usefont{T1}{pcr}{b}{n}\fontsize{36pt}{40pt}\selectfont\green}
\renewcommand{\familydefault}{\rmdefault}
\renewcommand{\rmdefault}{cmr}
\parindent 0pt
\parskip 5pt


\begin{document}


\begin{slide}{Today's Class}

\begin{itemize}
\item Bivariate data
\item Simple linear regression
\item Scatter plots
\item Correlation
\item Inference for regression
\item Inference for correlation

\end{itemize}

\end{slide}


%------------------------------------------

\begin{slide}{Bivariate Data}

\begin{itemize}
\item A dataset with two variables contains what is called bivariate data.

\item Each pair of values describes the same instance. For example, the first value of both variables describe the same individual or item.
    
\item It is often of interest to describe the relationship between two variables.
\end{itemize}
\end{slide}
%------------------------------------------

\begin{slide}{Data used today}

\begin{verbatim}
>x<-c(5.98, 8.80, 6.89, 8.49, 8.48, 7.47,
    7.97,5.94, 7.32, 6.64, 6.94, 3.51)
>
>
>y<-c(5.56, 7.80, 6.13, 8.15, 7.95, 7.87,
    8.03, 5.67, 7.11, 6.65, 7.02, 3.88)

\end{verbatim}
It is important the both data sets have same length.
\begin{verbatim}
>length(x)
>length(y)

\end{verbatim}
\end{slide}

%------------------------------------------
\begin{slide}{Regression Models}

\begin{itemize}
\item Simple linear regression (today).
    \begin{itemize}
    \item Estimates for slope and intercept.
    \item Accessing those estimates.
    \item Inference on those estimates.
    \end{itemize}
\item Multiple linear regression (later).
    \begin{itemize}
    \item Model selection
    \end{itemize}
\item Non-linear regression (later).
\item Diagnostics (later).
\end{itemize}
\end{slide}
%------------------------------------------
\begin{slide}{Simple linear regression}
\begin{itemize}
\item Simple linear regression is used to describe the relationship
between two variables `x' and `y'. \item For example, you may want to describe the
relationship between age and blood pressure or the relationship
between scores in a midterm exam and scores in the final exam,
etc.
\item `x' is the independent (i.e. predictor) variable
\item `y' is the dependent (i.e. response) variable.
\item Necessarily both x and y should be of equal length.

\item One of the first steps in a regression analysis is to determine if any
kind of relationship exists between `x' and `y'.



\end{itemize}
\end{slide}
%------------------------------------------
\begin{slide}{Simple linear regression}
\begin{itemize}

\item A scatterplot can created and can initially be used to get an idea
about the nature of the relationship between the variables, e.g. if
the relationship is linear, curvilinear, or no relationship exists.

\item We can see from a scatterplot that there is a linear relationship
between x and y.

\item Simple linear regression is only useful when there is evidences of a linear relationship.
In other cases, such as quadratic relationships, other types of regression may be useful.

\end{itemize}
\end{slide}
%------------------------------------------

\begin{slide}{Scatterplot}
\begin{itemize}
\item To make a simple scatterplot of the bivariate data, we simply use the
``plot()" command. \item The independent variable (the variable to go along the x-axis) is always specified first.
\begin{verbatim}
>plot(x,y)
\end{verbatim}
\item In future classes, we will look at how to improve and enhance scatter-plots, by controlling graphical parameters.
\end{itemize}
\end{slide}

%------------------------------------------
\begin{slide}{Correlation}
\begin{itemize}

\item The Pearson product-moment correlation coefficient is a measure of the strength of the linear relationship between two variables. \item  It is referred to as Pearson's correlation or simply as the correlation coefficient. \item If the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables.
\end{itemize}
\end{slide}

%------------------------------------------



\begin{slide}{Correlation}
\begin{itemize}
\item To compute the Pearson correlation coefficient (``r"), we use the ``cor()" command.
\begin{verbatim}
> cor(x,y)
[1] 0.9581898
\end{verbatim}
\item The coefficient should be between $-1$ and $1$.
\item Recall, the higher the absolute value of the correlation coefficient, the stronger the linear relationship.
\item A positive correlation coefficient indicates a positive relationship.
\item A negative correlation coefficient indicates a negative (inverse) relationship.
\end{itemize}
\end{slide}

\begin{slide}{Correlation and Covariance}
\begin{itemize}
\item Other types of correlation coefficient are possible, such as the Spearman coefficient, and the Kendall Tau coefficient.
\item To specify one of these methods, add the argument to the command, as shown below.
 \begin{verbatim}
> cor(x,y,method="kendall")
[1] 0.7878788
> cor(x,y,method="spearman")
[1] 0.909091
\end{verbatim}

\item To compute the covariance, we use the ``cov()" command.
\begin{verbatim}
> cov(x,y)
[1] 1.824429
\end{verbatim}

\end{itemize}
\end{slide}
%------------------------------------------

\begin{slide}{Simple Linear Regression}



\begin{itemize}
\item Basic regression model :
$y=\beta_{0} + \beta_{1}x + \epsilon$

\item
The intercept $\beta_{0}$ describes the point at which the line intersects
the y axis
\item The slope $\beta_{1}$ describes the change in `y'  for every unit increase in `x'.

\item From the data set, we determine the regression coefficients, i.e estimates for slope and intercept.

\begin{itemize}
\item
 $\hat{\beta}_{0}$ : the intercept estimate.
\item $\hat{\beta}_{1}$ : the slope estimate.
\end{itemize}
\item Fitted model : $\hat{y}=\hat{\beta}_{0} + \hat{\beta}_{1}x $
\end{itemize}


\end{slide}

%------------------------------------------

\begin{slide}{The lm() command.}
\begin{itemize}
\item The command lm() is used to fit linear models. \item Firstly the response variable is specified, then the predictor variable. \item The tilde sign is used to denote the dependent relationship (i.e. y depends on x).
\item The regression coefficients are then determined.
\end{itemize}
\begin{verbatim}
> lm(y~x)

Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x
     0.7812       0.8581
\end{verbatim}

\end{slide}
%------------------------------------------

\begin{slide}{Simple linear regression}
\begin{itemize} \item A more detailed model (i.e. more than just the coefficients) is generated in the form of a data object. \item We can give a name to the model, and view all of the results of the calculation, including \begin {itemize} \item The regression coefficients \item
The fitted $\hat{y}$ values (i.e. the estimated `y' values for the x date set) \item The residuals (i.e. the differences between  the estimated `y' values and the observed `y' values). \end{itemize}\item
In common with all data structures we can use the names() function and `\$' to access components.\end{itemize}
\end{slide}
%------------------------------------------

\begin{slide}{Simple linear regression}

\begin{verbatim}
> fit1 = lm(y~x)
>
> names(fit1)
 [1] "coefficients"  "residuals"
 [3] "effects"       "rank"
 [5] "fitted.values" "assign"
 [7] "qr"            "df.residual"
 [9] "xlevels"       "call"
[11] "terms"         "model"
>
>summary(fit1)
\end{verbatim}

\end{slide}

%------------------------------------------

\begin{slide}{Simple linear regression}
We can access components using the `\$'.
\begin{verbatim}
>
> fit1$coefficients
(Intercept)           x
  0.7812216   0.8580521

>
> fit1$coefficients[1]  #intercept
(Intercept)
  0.7812216
>
> fit1$coefficients[2] #slope
       x
0.8580521
\end{verbatim}

\end{slide}
%------------------------------------------

\begin{slide}{Simple Linear Regression}
An alternative method is to use the following commands.
\begin{itemize}
\item coef() - returns the regression coefficients of the model.
\item fitted() - returns the fitted values of the model.
\item resid() - returns the residuals of the model.


\end{itemize}
 \begin{verbatim}
 > coef(fit1)
(Intercept)           x
  0.7812216   0.8580521
\end{verbatim}


\end{slide}

%------------------------------------------

\begin{slide}{Coefficient of Determination}
\begin{itemize}
\item The coefficient of determination $R^2$ is  the proportion of variability in a data set
that is accounted for by the linear model.
\item $R^2$ provides a measure of how well future outcomes are likely to be predicted by the model.
\item For simple linear regression, it can also be computed by squaring the correlation coefficient.
\end{itemize}

\begin{verbatim}
> summary(fit1)$r.squared
[1] 0.9181277
\end{verbatim}


\end{slide}

%------------------------------------------

\begin{slide}{p-values}
\begin{itemize}
\item
We will begin to use hypothesis testing in out analyses.
\item We will mostly be using ``p-values".
\item If the p-value is very low, we reject the null hypothesis.
\item If it is above an arbitrary threshold, we ``fail to reject" the null hypothesis.
\item We will use 0.01 (1\%) as our arbitrary threshold.
\item The relevant hypotheses will be discussed for each methodology.
\end{itemize}
\end{slide}

%------------------------------------------

\begin{slide}{Inference for regression}
\begin{itemize}
\item
We can use the ``summary()" command to determine
test statistics and p-values for the both
regression coefficients.
\item In both cases the null hypothesis is that the true value is zero.
\item Consequently the alternative hypothesis is that they are not zero in both cases.
\item Stating that the slope is zero is equivalent to saying that there is no relationship between `x' and `y'.
\end{itemize}
\end{slide}
%------------------------------------------

\begin{slide}{Inference for regression}
\begin{verbatim}
> summary(fit1)

Call:
lm(formula = y ~ x)

Residuals:
     Min       1Q   Median       3Q      Max
-0.56320 -0.24413  0.06588  0.19946  0.67913

Coefficients:
        Estimate Std. Error t value Pr(>|t|)
(Int.)  0.78122    0.58121   1.344    0.209
x       0.85805    0.08103  10.590 9.38e-07 ***

.....

\end{verbatim}
\end{slide}
%------------------------------------------

\begin{slide}{Inference for regression}
\begin{itemize}
\item The p value for the intercept is 0.209. This means we fail to reject the null hypothesis that true intercept is zero.
\item The p value for the slope is extremely small. This means we reject the null hypothesis that it is zero.
\item Consequently we reject the hypothesis that there is no relationship between `x' and `y'.
\item Notice that the stars beside the p-value. The more stars, the lower the p-value.
\end{itemize}
\end{slide}


%------------------------------------------
\begin{slide}{Inference for correlation}
\begin{itemize}

\item The cor() function in R can be extended to provide the significance testing required. 
\item The function is ``cor.test()".
\begin{verbatim}
> cor(x,y)
[1] 0.9581898

\end{verbatim}
\item For this test, the null hypothesis is that the true correlation coefficient is zero.
\item The alternative is that the true value is not zero, and a linear relationship exists.
\end{itemize}
\end{slide}


%------------------------------------------
\begin{slide}{Inference for correlation (contd)}
\begin{verbatim}

> cor.test(x,y)

        Pearson's product-moment correlation

data:  x and y
t = 10.5897, df = 10, p-value = 9.379e-07
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.8537967 0.9885056
sample estimates:
      cor
0.9581898 

\end{verbatim}
\end{slide}

%------------------------------------------
\begin{slide}{Inference for correlation (contd)}
\begin{itemize}
\item The p-value for this hypothesis test is 9.379e-07.
\item As this is much smaller than out threshold of 0.01, we reject the null hypothesis.
\item There is a linear relationship between x and y.
\item Notice the alternative hypothesis was expressed in the output.
\item Also notice that a 95\% confidence interval was given for the correlation coefficient.
\end{itemize}
\end{slide}
\end{document}

